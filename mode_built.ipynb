{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Final_dataset.csv\")\n",
    "X = data.drop(columns=['target']).values\n",
    "y = data['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Accuracy: 0.94\n",
      "Confusion Matrix:\n",
      "[[4980  267]\n",
      " [ 295 4458]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      5247\n",
      "           1       0.94      0.94      0.94      4753\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Random_Forst_model.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_rf = GridSearchCV(random_forest_model, param_grid, cv=stratified_kfold, scoring='f1_macro', verbose=1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters:', grid_rf.best_params_)\n",
    "\n",
    "y_pred = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    " \n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{classification_rep}')\n",
    "\n",
    "model_filename = \"Random_Forst_model.joblib\"\n",
    "joblib.dump(grid_rf.best_estimator_, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1620 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1179, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 73, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.89032966        nan 0.89043    0.88876489\n",
      " 0.88933999 0.8895144  0.88949057 0.88981763        nan        nan\n",
      "        nan        nan        nan 0.88452702 0.88417558        nan\n",
      " 0.88951703 0.88969312        nan        nan 0.89035512        nan\n",
      " 0.89043    0.88876489 0.88953055 0.8895144  0.88943865 0.88944037\n",
      "        nan        nan        nan        nan        nan 0.88452702\n",
      " 0.88480749        nan 0.88895709 0.8894936         nan        nan\n",
      " 0.89035512        nan 0.89042999 0.88876489 0.88871335 0.8895144\n",
      " 0.88960663 0.88946411        nan        nan        nan        nan\n",
      "        nan 0.88452702 0.88491149        nan 0.88815953 0.88905884\n",
      "        nan        nan 0.89032966        nan 0.89032966 0.8896206\n",
      " 0.88952145 0.88956977 0.88941826 0.88972066        nan        nan\n",
      "        nan        nan        nan 0.88452231 0.88380012        nan\n",
      " 0.88967001 0.88979565        nan        nan 0.89032967        nan\n",
      " 0.89035512 0.8896206  0.88957046 0.88956977 0.88937046 0.8893434\n",
      "        nan        nan        nan        nan        nan 0.88452231\n",
      " 0.8844527         nan 0.88888663 0.88964525        nan        nan\n",
      " 0.89035512        nan 0.89030421 0.8896206  0.88957046 0.88956977\n",
      " 0.88959592 0.88942125        nan        nan        nan        nan\n",
      "        nan 0.88452231 0.8846734         nan 0.88744475 0.88888805\n",
      "        nan        nan 0.89027959        nan 0.889952   0.88781955\n",
      " 0.88664207 0.88732747 0.88951706 0.88971858        nan        nan\n",
      "        nan        nan        nan 0.88452702 0.88417558        nan\n",
      " 0.88941668 0.88961696        nan        nan 0.89032913        nan\n",
      " 0.88992192 0.88781955 0.88737781 0.88732747 0.88905897 0.88949294\n",
      "        nan        nan        nan        nan        nan 0.88452702\n",
      " 0.88480749        nan 0.88900933 0.88954249        nan        nan\n",
      " 0.89032913        nan 0.88991682 0.88781955 0.88777984 0.88732747\n",
      " 0.88888662 0.88911045        nan        nan        nan        nan\n",
      "        nan 0.88452702 0.88491149        nan 0.88808526 0.88905885\n",
      "        nan        nan 0.89035389        nan 0.88995479 0.88706707\n",
      " 0.88656144 0.88711729 0.88959644 0.88981978        nan        nan\n",
      "        nan        nan        nan 0.88452231 0.88380012        nan\n",
      " 0.88956964 0.88979568        nan        nan 0.89032913        nan\n",
      " 0.89017545 0.88706707 0.8871467  0.88711729 0.88911747 0.8895449\n",
      "        nan        nan        nan        nan        nan 0.88452231\n",
      " 0.8844527         nan 0.88891343 0.88967064        nan        nan\n",
      " 0.89032913        nan 0.89015159 0.88706707 0.8872666  0.88711729\n",
      " 0.88844548 0.88914293        nan        nan        nan        nan\n",
      "        nan 0.88452231 0.8846734         nan 0.88747024 0.88886331\n",
      "        nan        nan 0.88636599        nan 0.88997097 0.88489978\n",
      " 0.88477849 0.8848528  0.88951762 0.8896678         nan        nan\n",
      "        nan        nan        nan 0.88452702 0.88417558        nan\n",
      " 0.88946816 0.88969319        nan        nan 0.88636599        nan\n",
      " 0.88961872 0.88489978 0.8850831  0.8848528  0.88908491 0.88939256\n",
      "        nan        nan        nan        nan        nan 0.88452702\n",
      " 0.88480749        nan 0.88900863 0.88946821        nan        nan\n",
      " 0.88641624        nan 0.88921428 0.88489978 0.88518718 0.8848528\n",
      " 0.88818497 0.88895776        nan        nan        nan        nan\n",
      "        nan 0.88452702 0.88491149        nan 0.88811073 0.88905819\n",
      "        nan        nan 0.88628359        nan 0.889898   0.88525235\n",
      " 0.88488497 0.88539967 0.88959373 0.8898204         nan        nan\n",
      "        nan        nan        nan 0.88452231 0.88380012        nan\n",
      " 0.88977161 0.88979635        nan        nan 0.88628359        nan\n",
      " 0.88967144 0.88525235 0.88545029 0.88539967 0.8888867  0.88969607\n",
      "        nan        nan        nan        nan        nan 0.88452231\n",
      " 0.8844527         nan 0.88888805 0.88964588        nan        nan\n",
      " 0.88628359        nan 0.88926962 0.88525235 0.88484602 0.88539967\n",
      " 0.88757142 0.88883789        nan        nan        nan        nan\n",
      "        nan 0.88452231 0.8846734         nan 0.8875445  0.88886331\n",
      "        nan        nan 0.88460595        nan 0.88971858 0.88455324\n",
      " 0.88409807 0.88460354 0.8894668  0.88969253        nan        nan\n",
      "        nan        nan        nan 0.88452702 0.88417558        nan\n",
      " 0.88944275 0.88966774        nan        nan 0.88463133        nan\n",
      " 0.88956782 0.88455324 0.88488046 0.88460354 0.88898259 0.88944281\n",
      "        nan        nan        nan        nan        nan 0.88452702\n",
      " 0.88480749        nan 0.8889839  0.88946757        nan        nan\n",
      " 0.8846562         nan 0.88898463 0.88455324 0.8849079  0.88460354\n",
      " 0.88810942 0.889008          nan        nan        nan        nan\n",
      "        nan 0.88452702 0.88491149        nan 0.88813548 0.88905953\n",
      "        nan        nan 0.88490251        nan 0.88989611 0.88474859\n",
      " 0.88425764 0.88484872 0.88974555 0.88974476        nan        nan\n",
      "        nan        nan        nan 0.88452231 0.88380012        nan\n",
      " 0.88964386 0.88977089        nan        nan 0.88492791        nan\n",
      " 0.88967056 0.88474859 0.88460101 0.88484872 0.88893818 0.88967064\n",
      "        nan        nan        nan        nan        nan 0.88452231\n",
      " 0.8844527         nan 0.88891279 0.88969603        nan        nan\n",
      " 0.88492791        nan 0.88896365 0.88474859 0.88437203 0.88484872\n",
      " 0.88752116 0.88886331        nan        nan        nan        nan\n",
      "        nan 0.88452231 0.8846734         nan 0.88754598 0.88886331\n",
      "        nan        nan 0.88450221        nan 0.88969248 0.88452702\n",
      " 0.88409866 0.88465495 0.88946753 0.88969319        nan        nan\n",
      "        nan        nan        nan 0.88452702 0.88417558        nan\n",
      " 0.88944282 0.88969319        nan        nan 0.88452767        nan\n",
      " 0.88946821 0.88452702 0.8846292  0.88465495 0.88898393 0.88944212\n",
      "        nan        nan        nan        nan        nan 0.88452702\n",
      " 0.88480749        nan 0.8890093  0.88944281        nan        nan\n",
      " 0.88452767        nan 0.88900937 0.88452702 0.88490583 0.88465495\n",
      " 0.88811073 0.88903478        nan        nan        nan        nan\n",
      "        nan 0.88452702 0.88491149        nan 0.88813478 0.88903338\n",
      "        nan        nan 0.88457261        nan 0.88982104 0.88454708\n",
      " 0.88365095 0.88472279 0.88966997 0.8898204         nan        nan\n",
      "        nan        nan        nan 0.88452231 0.88380012        nan\n",
      " 0.88964454 0.8897702         nan        nan 0.88457261        nan\n",
      " 0.88964525 0.88454708 0.88495867 0.88472279 0.88891353 0.88964463\n",
      "        nan        nan        nan        nan        nan 0.88452231\n",
      " 0.8844527         nan 0.88891272 0.88962049        nan        nan\n",
      " 0.88457261        nan 0.88888805 0.88454708 0.88487287 0.88472279\n",
      " 0.88746952 0.88888805        nan        nan        nan        nan\n",
      "        nan 0.88452231 0.8846734         nan 0.88754519 0.88888805\n",
      "        nan        nan 0.88455243        nan 0.88969317 0.88452702\n",
      " 0.88409872 0.88457788 0.88946753 0.88966774        nan        nan\n",
      "        nan        nan        nan 0.88452702 0.88417558        nan\n",
      " 0.88949297 0.88971858        nan        nan 0.88455243        nan\n",
      " 0.8894936  0.88452702 0.88475605 0.88457788 0.88905945 0.88949291\n",
      "        nan        nan        nan        nan        nan 0.88452702\n",
      " 0.88480749        nan 0.88893299 0.88944149        nan        nan\n",
      " 0.88452697        nan 0.88893371 0.88452702 0.88500931 0.88457788\n",
      " 0.88813548 0.88905813        nan        nan        nan        nan\n",
      "        nan 0.88452702 0.88491149        nan 0.88811073 0.88908428\n",
      "        nan        nan 0.88452154        nan 0.88974476 0.88452231\n",
      " 0.88354893 0.88459718 0.88979573 0.88972005        nan        nan\n",
      "        nan        nan        nan 0.88452231 0.88380012        nan\n",
      " 0.88964585 0.8898204         nan        nan 0.88454708        nan\n",
      " 0.88969609 0.88452231 0.88466909 0.88459718 0.8888626  0.88969603\n",
      "        nan        nan        nan        nan        nan 0.88452231\n",
      " 0.8844527         nan 0.8889643  0.88967064        nan        nan\n",
      " 0.88457255        nan 0.88886331 0.88452231 0.88457104 0.88459718\n",
      " 0.88754665 0.88888805        nan        nan        nan        nan\n",
      "        nan 0.88452231 0.8846734         nan 0.88749572 0.88886331]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'C': 0.001, 'fit_intercept': True, 'max_iter': 50, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best Cross-Validated Accuracy: 89.04%\n",
      "Accuracy: 0.89\n",
      "Confusion Matrix:\n",
      "[[4743  504]\n",
      " [ 570 4183]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      5247\n",
      "           1       0.89      0.88      0.89      4753\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LogisticRegression_model.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'fit_intercept': [True, False],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [50, 100, 200],\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated Accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{classification_rep}')\n",
    "model_filename = \"LogisticRegression_model.joblib\"\n",
    "joblib.dump(grid_search, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "540 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\VASU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8336933  0.78766585 0.82722234 0.78896633 0.83981701 0.78404302\n",
      " 0.83964019 0.78310761 0.83773911 0.7783187  0.84342829 0.78699103\n",
      " 0.82763877 0.78170502 0.82763877 0.78170502 0.8519157  0.79058405\n",
      " 0.83047725 0.77896484 0.82652666 0.79020787 0.822295   0.77813708\n",
      " 0.8246042  0.78493332 0.81070838 0.79821484 0.81718137 0.78787759\n",
      " 0.81227666 0.78755755 0.81227666 0.78755755 0.81079849 0.76644663\n",
      " 0.93723197 0.91974867 0.93660989 0.92154377 0.93741166 0.91657023\n",
      " 0.93516955 0.91904792 0.93590254 0.91938937 0.93790948 0.92159862\n",
      " 0.93693257 0.92263462 0.93693257 0.92263462 0.93868055 0.9234654\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.78546593 0.68028237 0.79007285 0.70482271 0.78913033 0.72174894\n",
      " 0.80275422 0.69351146 0.80140334 0.69750711 0.79558335 0.69829052\n",
      " 0.77881905 0.69438036 0.77881905 0.69438036 0.79432196 0.71754264\n",
      " 0.76580783 0.67202444 0.78150672 0.73109506 0.77688216 0.70304997\n",
      " 0.75946361 0.69290019 0.75989652 0.68693457 0.77635757 0.68161609\n",
      " 0.79061835 0.70304935 0.79061835 0.70304935 0.7774628  0.67685642\n",
      " 0.94345951 0.882421   0.9434071  0.88358631 0.94350523 0.87707399\n",
      " 0.94304564 0.88256825 0.94306945 0.88351135 0.94362426 0.88317904\n",
      " 0.94317716 0.874135   0.94317716 0.874135   0.94333224 0.87628772\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.83517955 0.76777262 0.83182277 0.75890052 0.829986   0.77945738\n",
      " 0.83616447 0.76258924 0.83409482 0.76722835 0.84486527 0.77744769\n",
      " 0.84014465 0.75988315 0.84014465 0.75988315 0.82719113 0.76753219\n",
      " 0.82593634 0.74388807 0.81993494 0.76219236 0.82576071 0.76771976\n",
      " 0.81783431 0.75187968 0.82024482 0.77417993 0.81970983 0.76602148\n",
      " 0.81385444 0.7533812  0.81385444 0.7533812  0.81992147 0.74844425\n",
      " 0.93711628 0.92049287 0.93729165 0.9224585  0.9376395  0.91768098\n",
      " 0.93630987 0.91919139 0.93678721 0.92123327 0.93786254 0.91783331\n",
      " 0.93758526 0.92023473 0.93758526 0.92023473 0.93822806 0.9229127\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8371864  0.79627424 0.84365369 0.79040045 0.84603804 0.79545515\n",
      " 0.82558926 0.78101599 0.83789793 0.79592082 0.84202901 0.79266449\n",
      " 0.84444537 0.78910029 0.84444537 0.78910029 0.82557726 0.7858529\n",
      " 0.83596265 0.79461463 0.82093593 0.77664008 0.82279546 0.78402109\n",
      " 0.82062167 0.77512213 0.82844244 0.77621727 0.82130706 0.78789435\n",
      " 0.83362104 0.77657222 0.83362104 0.77657222 0.82235036 0.77208778\n",
      " 0.9358876  0.92210174 0.93548385 0.92225754 0.93601117 0.92376566\n",
      " 0.93453012 0.9247354  0.93453468 0.92342716 0.93658417 0.92282852\n",
      " 0.9362098  0.9207574  0.9362098  0.9207574  0.93604628 0.92292044\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.79709106 0.70205253 0.76843064 0.70250864 0.80817373 0.67600083\n",
      " 0.77421102 0.67345029 0.79043847 0.70564045 0.79816626 0.71332264\n",
      " 0.80786677 0.69320268 0.80786677 0.69320268 0.79667843 0.69728142\n",
      " 0.77786618 0.70839984 0.77534052 0.69034259 0.78508183 0.6875001\n",
      " 0.75753537 0.68124524 0.78496669 0.69339673 0.78176833 0.65631396\n",
      " 0.75344602 0.68960525 0.75344602 0.68960525 0.77044145 0.70996324\n",
      " 0.94164105 0.88838281 0.94201448 0.88975589 0.94216238 0.89456093\n",
      " 0.94200679 0.87909544 0.94253661 0.87947816 0.94241281 0.89191118\n",
      " 0.94231523 0.89077787 0.94231523 0.89077787 0.9424937  0.88567292\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84549417 0.79214508 0.85180815 0.77048644 0.828733   0.76532271\n",
      " 0.83332662 0.76253581 0.8459432  0.76390873 0.83615072 0.77888128\n",
      " 0.82739007 0.7701391  0.82739007 0.7701391  0.84445121 0.77130862\n",
      " 0.83395259 0.7524581  0.84095571 0.75646299 0.81708844 0.76351104\n",
      " 0.82657274 0.76959741 0.82609173 0.7644158  0.82151968 0.76817793\n",
      " 0.83344688 0.76139447 0.83344688 0.76139447 0.80568181 0.75979611\n",
      " 0.93659217 0.92415502 0.93538391 0.92378619 0.9357632  0.92271195\n",
      " 0.93473466 0.9191925  0.93460679 0.92046808 0.93723912 0.92159229\n",
      " 0.93636057 0.92211941 0.93636057 0.92211941 0.93670244 0.92269194]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'random_state': 42, 'splitter': 'best'}\n",
      "Accuracy: 0.95\n",
      "Confusion Matrix:\n",
      "[[5001  246]\n",
      " [ 292 4461]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      5247\n",
      "           1       0.95      0.94      0.94      4753\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Decision_tree_model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'random_state': [42], \n",
    "}\n",
    "\n",
    "dt_model = tree.DecisionTreeClassifier()\n",
    "\n",
    "grid_dt = GridSearchCV(dt_model, param_grid, cv=stratified_kfold, scoring='f1_macro', verbose=1)\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters:', grid_dt.best_params_)\n",
    "\n",
    "y_pred = grid_dt.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{classification_rep}')\n",
    "model_filename = \"Decision_tree_model.joblib\"\n",
    "joblib.dump(grid_dt, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'algorithm': 'SAMME.R', 'estimator': DecisionTreeClassifier(max_depth=2), 'learning_rate': 1, 'n_estimators': 200, 'random_state': 42}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Adaboost_model.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'estimator': [tree.DecisionTreeClassifier(max_depth=1), tree.DecisionTreeClassifier(max_depth=2)],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'random_state': [42], \n",
    "}\n",
    "\n",
    "adaboost = AdaBoostClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=adaboost, param_grid=param_grid, cv=stratified_kfold, scoring='f1_macro')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_adaboost = grid_search.best_estimator_\n",
    "y_pred = best_adaboost.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "model_filename = \"Adaboost_model.joblib\"\n",
    "joblib.dump(best_adaboost, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
